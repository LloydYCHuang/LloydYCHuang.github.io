---
layout: default
categories: Statistic
title:  "Support vector machine, SVM (支援向量機演算法)"
---  
## Support vector machine, SVM (支援向量機演算法)   
2021/09/09  
SVM也是一個很常見的演算法，網誌的統計篇也越來越充實了，而且目前最廣泛使用的SVM演算法是臺大林智仁教授開發的libsvm，對應到R套件就是e1071，這方面的資源非常非常的多，這裡簡單整理我的自學筆記。  
可以參考<a href="https://rpubs.com/skydome20/R-Note14-SVM-SVR" target="_blank">R筆記</a>、<a href="https://cran.r-project.org/web/packages/e1071/e1071.pdf" target="_blank">e1071的套件說明</a>等。  
  
### 何謂SVM  
SVM是一種監督式學習 (supervised learning) 演算法，最早由Cortes and Vapnik (1995) 提出，現在引用數也接近五萬次，是一種二元分類器 (binary classifier)。  
所謂二元分類器的概念是非常簡單的，如果要用線性的方法區分兩種不同標籤的資料，即用一條直線把兩種標籤的資料區分出來，可以表示如圖1這樣：   
<img src="https://lloydychuang.github.io/assets/RF1.jpg" width="700">   
當資料越來越複雜，要用線性的方法區分兩組資料就越來越困難，有時根本沒辦法區分，如圖2(a)，以直線根本分不出來吧。
圖2   
如果兩個維度分不出來，那就用三個吧！把資料映射到一個**更高維**的空間中，可以讓其更為有效地被區分，讓我們回想高中數學的點距離方程式，在圖2(a) 的中間給定一個點 (3,3)，並計算其他點到點 (3,3) 的距離$$\sqrt{(x-3)^2+(y-3)^2}$$，並把$$\sqrt{(x-3)^2+(y-3)^2}$$作為圖2(a) 的第三維Z座標繪圖，可以得到圖2(b) 的結果。離 (3,3) 較近的綠色點就會有比較小的Z座標，反之橘色點的Z座標比較大，在圖2(b) 中兩種標籤被很明顯的區隔出來，要用線性的方法區分也很容易，SVM的原理就是以先將資料映射到更高維的空間，再進行二元分類。  
  
當兩種標籤的樣品要被區分出來時，SVM會尋找一個超平面 (hyperplane) 來區隔兩組資料，如圖3。  
圖3  
由於超平面是在比原本資料更高維度中，所以在超平面看起來並不一定是直線，但這裡先以直線表示。SVM計算該平面到兩種分類中各自最接近的點的距離，稱作邊界 (margin)，平面會與兩邊界保持相同的距離，若邊界越大就代表越能把兩種標籤區分出來，圖3(a) 的邊界就比圖3(b) 來的大，採用圖3(a) 的超平面就更容易區分資料，以下介紹實際的示範。  
  
### 尋找超平面  
假設有一個超空間是由$$x_1, x_2, x_3$$三組向量組成，如圖4，這個空間中的平面可以表示成這樣：  
$$\begin{align}
\begin{bmatrix}x_1&x_2&x_3\end{bmatrix} 
\begin{bmatrix}\beta_1\\\beta_2\\\beta_3\end{bmatrix} + \beta_0 &= 0 \\
x^T \beta + \beta_0 &= 0 \\
\end{align}$$  
圖4  
既然分類的超平面是$$x^T \beta + \beta_0 = 0$$，那麼兩組標籤的分類就分別為$$x^T \beta + \beta_0 > 0$$及x^T \beta + \beta_0 < 0$$在平面的兩邊，代入方程式會有不同的正負以判別類別。  
接著，需要找到$$x^T$$和$$\beta_0$$讓這個平面到兩種標籤的邊界最大，以M代表邊界的大小，因為邊界有兩邊所以使用2M (圖5)，其中$$\beta$$即是與這個平面垂直的單位向量 ($$\begin{Vmatrix}\beta\end{Vmatrix} =1$$)，如圖5中的紅色箭頭。  
圖5  
讓我們假設平面與兩種標籤距離最近的點接觸時，會出現的平面為$$x^T \beta + \beta_0 > k$$及x^T \beta + \beta_0 < -k$$ (合理吧，在兩邊)，如果我們可以找到一個跟$$\beta$$平行的向量，即使不是單位向量也行，就可以計算出超平面了。因此替代的方法就是**從一個邊界到另一個邊界的向量W**，其長度可表示為  
$$\begin{Vmatrix}W\end{Vmatrix} =2M$$  

因此替代的方案就是圖6
計算各點到平面的距離


SVM接著計算平面到若邊界，就代表越能把兩種分類區分出來，如圖3。  
  
如果是圖1(a)的樣子，當然很好分群，但如果是圖1(b)的樣子呢？這個問題後面再來說，

Cortes, C., Vapnik, V. 1995. Support-vector networks. Machine Learning, 20, 273–297. <a href="https://doi.org/10.1007/BF00994018" target="_blank">https://doi.org/10.1007/BF00994018</a>
