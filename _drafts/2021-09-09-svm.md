---
layout: default
categories: Statistic
title:  "Support vector machine, SVM (支援向量機演算法)"
---  
## Support vector machine, SVM (支援向量機演算法)   
2021/09/09  
SVM也是一個很常見的演算法，網誌的統計篇也越來越充實了，而且目前最廣泛使用的SVM演算法是臺大林智仁教授開發的libsvm，對應到R套件就是e1071，這方面的資源非常非常的多，這裡整理我的自學筆記。  
可以參考<a href="https://rpubs.com/skydome20/R-Note14-SVM-SVR" target="_blank">R筆記</a>、<a href="https://cran.r-project.org/web/packages/e1071/e1071.pdf" target="_blank">e1071的套件說明</a>等。  
  
### 何謂SVM  
SVM是一種監督式學習 (supervised learning) 演算法，最早由Cortes and Vapnik (1995) 提出，現在引用數也接近五萬次，是一種二元分類器 (binary classifier)。  
所謂二元分類器的概念是非常簡單的，如果要用線性的方法區分兩種不同標籤的資料，可以表示如圖1這樣：   
<img src="https://lloydychuang.github.io/assets/RF1.jpg" width="700">   
當資料越來越複雜，要用線性的方法區分兩組資料就越來越困難，有時根本沒辦法區分，如圖2(a)，以直線根本分不出來吧。
圖2   
此時SVM的做法是這樣，如果兩個維度分不出來，那就用三個吧！把資料映射到一個更高維的空間中，讓其更為有效地被區分，讓我們回想高中數學的點距離方程式，在圖2(a) 的中間給定一個點 (3,3)，並計算其他點到點 (3,3) 的距離$$\sqrt{(x-3)^2+(y-3)^2}$$，把$$\sqrt{(x-3)^2+(y-3)^2}$$作為圖2(a) 的第三維座標繪圖，可以得到圖2(b) 的結果。在圖2(b) 中兩種標籤被很明顯的區隔出來，要用線性的方法區分也很容易，SVM就是以先將資料映射再區隔的方式進行。  
  
當我們想要區分兩組資料時，SVM會尋找一個更高維度的超平面 (hyperplane) 來區隔兩組資料，如圖3。  
圖3  
由於這個平面是在比原本資料更高維度中，所以在原本資料的維度並不一定是直線，但這裡先以直線表示。SVM計算該平面到兩種分類中各自最接近的點的距離，稱作邊界 (margin)，平面會與兩邊界保持相同的距離，若邊界越大就代表越能把兩種標籤區分出來，圖3(a) 的邊界就比圖3(b) 來的大，採用圖3(a) 的超平面就更容易區分資料。  
  
### 尋找超平面  
假設有一個超空間是由$$x_1, x_2, x_3$$三組向量組成，如圖4，這個空間中的平面可以表示成這樣：  
$$\begin{align}
\begin{bmatrix}x_1&x_2&x_3\end{bmatrix} 
\begin{bmatrix}\beta_1\\\beta_2\\\beta_3\end{bmatrix} + \beta_0 &= 0 \\
x^T \beta + \beta_0 &= 0 \\
\end{align}$$  
圖4  
並且計算各點到平面的距離
超平面在空間中的方程式可以表示為
在計算超平面到各點邊界的時候，

SVM接著計算平面到若邊界，就代表越能把兩種分類區分出來，如圖3。  
  
如果是圖1(a)的樣子，當然很好分群，但如果是圖1(b)的樣子呢？這個問題後面再來說，

Cortes, C., Vapnik, V. 1995. Support-vector networks. Machine Learning, 20, 273–297. <a href="https://doi.org/10.1007/BF00994018" target="_blank">https://doi.org/10.1007/BF00994018</a>
