---
layout: default
categories: Statistic
title:  "Support vector machine, SVM (支援向量機演算法)"
---  
## Support vector machine, SVM (支援向量機演算法)   
2021/09/09  
SVM也是一個很常見的演算法，網誌的統計篇也越來越充實了，而且目前最廣泛使用的SVM演算法是臺大林智仁教授開發的libsvm，對應到R套件就是e1071，這方面的資源非常非常的多，這裡簡單整理我的自學筆記。  
可以參考<a href="https://rpubs.com/skydome20/R-Note14-SVM-SVR" target="_blank">R筆記</a>、<a href="https://cran.r-project.org/web/packages/e1071/e1071.pdf" target="_blank">e1071的套件說明</a>等。  
  
### 何謂SVM  
SVM是一種監督式學習 (supervised learning) 演算法，最早由Cortes and Vapnik (1995) 提出，現在引用數也接近五萬次，是一種二元分類器 (binary classifier)。  
所謂二元分類器的概念是非常簡單的，如果要用線性的方法區分兩種不同標籤的資料，即用一條直線把兩種標籤的資料區分出來，可以表示如圖1這樣：   
<img src="https://lloydychuang.github.io/assets/RF1.jpg" width="700">   
當資料越來越複雜，要用線性的方法區分兩組資料就越來越困難，有時根本沒辦法區分，如圖2(a)，以直線根本分不出來吧。
圖2   
如果兩個維度分不出來，那就用三個吧！把資料映射到一個**更高維**的空間中，可以讓其更為有效地被區分，讓我們回想高中數學的點距離方程式，在圖2(a) 的中間給定一個點 (3,3)，並計算其他點到點 (3,3) 的距離$$\sqrt{(x-3)^2+(y-3)^2}$$，並把$$\sqrt{(x-3)^2+(y-3)^2}$$作為圖2(a) 的第三維Z座標繪圖，可以得到圖2(b) 的結果。離 (3,3) 較近的綠色點就會有比較小的Z座標，反之橘色點的Z座標比較大，在圖2(b) 中兩種標籤被很明顯的區隔出來，要用線性的方法區分也很容易，SVM的原理就是以先將資料映射到更高維的空間，再進行二元分類。  
  
當兩種標籤的樣品要被區分出來時，SVM會尋找一個超平面 (hyperplane) 來區隔兩組資料，如圖3。  
圖3  
由於超平面是在比原本資料更高維度中，所以在超平面看起來並不一定是直線，但這裡先以直線表示。SVM計算該平面到兩種分類中各自最接近的點的距離，稱作邊界 (margin)，平面會與兩邊界保持相同的距離，若邊界越大就代表越能把兩種標籤區分出來，圖3(a) 的邊界就比圖3(b) 來的大，採用圖3(a) 的超平面就更容易區分資料，以下介紹實際的示範。  
  
### 尋找超平面  
假設有一個超空間是由$$x_1, x_2, x_3$$三組向量組成，這個空間中的平面可以表示成這樣：  
$$\begin{align}
\begin{bmatrix}w_1&w_2&w_3\end{bmatrix} 
\begin{bmatrix}x_1\\x_2\\x_3\end{bmatrix} + \beta_0 &= 0 \\
w^T x + \beta_0 &= 0 \\
\end{align}$$  
圖4  
如圖4，$$w$$即是與這個平面垂直的向量，既然分類的超平面是$$w^T x + \beta_0 = 0$$，那麼在平面兩邊的兩組標籤就分別為$$w^T x + \beta_0 > 0$$及$$w^T x + \beta_0 < 0$$，代入方程式會有不同的正負予以判別。  
在圖4中，兩種標籤裡距離平面最近的點稱作support vector (支援向量)，support vector們與平面的距離決定了邊界的大小M (圖5)，兩邊的平面為$$w^T x + \beta_0 = 1$$和$$w^T x + \beta_0 = -1$$。
圖5  
現在假設在下平面的support vector為$$x_0$$，則$$x_0$$滿足$$w^T x_0 + \beta_0 = -1$$，在上平面的support vector為$$x_p$$，向量$$\overrightarrow{x_0x_p}$$連接這兩個support vector，因此我們只要計算$$\overrightarrow{x_0x_p}$$投影到$$w$$的向量即可！  
圖6  
把$$x_0$$對面的那個點稱為$$x'_0$$，接著計算

2M=\overrightarrow{x_0x_p} \cdot \frac{w}{ \begin{Vmatrix}w\end{Vmatrix} }\\


因此替代的方案就是圖6
計算各點到平面的距離


SVM接著計算平面到若邊界，就代表越能把兩種分類區分出來，如圖3。  
  
如果是圖1(a)的樣子，當然很好分群，但如果是圖1(b)的樣子呢？這個問題後面再來說，

Cortes, C., Vapnik, V. 1995. Support-vector networks. Machine Learning, 20, 273–297. <a href="https://doi.org/10.1007/BF00994018" target="_blank">https://doi.org/10.1007/BF00994018</a>
