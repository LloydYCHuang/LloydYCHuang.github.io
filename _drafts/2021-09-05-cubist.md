---
layout: default
categories: Statistic
title:  "Cubist and decision tree (Cubist演算法與決策樹)"
---  
## Cubist and decision tree (Cubist演算法與決策樹)   
2021/09/05   
Cubist是一個在土壤科學 (尤其是數位化土壤繪圖) 相當常見的演算法，他的演算方法是tree-based，所以其實是決策樹 (decision tree) 的延伸，這篇文章從決策樹開始講起，最後帶到這個機器學習上常見的Cubist模型。  
Cubist和其他決策樹模型一樣，都是出自Ross Quinlan這位澳洲學者，演算法包括ID3、C4.5、C5.0等都被廣泛使用 (ID3甚至從1993就開始在使用)，他創立的公司<a href="https://www.rulequest.com/index.html" target="_blank">RuleQuest</a>也將這些方法使用在資料處理上。  
   
*Shoot for the moon. Even if you miss, you'll land among the stars.   
&mdash;Les Brown*   
   
### 決策樹 (decision tree)  
決策樹演算法是監督式學習 (supervised learning) 的一種，監督式學習表示事先給定資料的標籤，再由演算法進行學習並嘗試預測，白話一點來說，就是教導電腦什麼是鯊魚，什麼是鯨魚，然後再讓電腦看圖辨別魚類，反過來說非監督式學習 (unsupervised learning) 就是未給定資料標籤，讓電腦自己從不同魚類照片中依照形態、大小、顏色等特徵來區分。機器學習的監督式學習通常會輸入一組含有許多變數 (特徵) 的資料，例如我們假設一組動物資料。  
  
| 動物 | 有翅膀 | 會游泳 | 會飛 |     
| :---: | :---: | :---: | :---: |      
| 駱駝 | F | F | F |       
| 鯊魚 | F | T | F |      
| 企鵝 | T | T | F |      
| 奇異鳥 | T | F | F |       
{: .tablelines}    
   
決策樹的概念相當簡單，以二元區分的方法，一個節點 (node) 代表一項判別，延伸出兩個分支 (符合與否)，分支又再到下一個節點，直到區分出所有的資料 (圖1)。  
(圖1)  
隨之而來的問題是，我們怎麼挑選節點，如圖1中可以看到"會飛"這個項目完全沒有用到，因為四種動物都不會飛，所以不需要用到這個節點，而"有翅膀"和"會游泳"其實都是很能區分動物的節點，而決策樹選擇節點的方法是透過**不純度 (impurity)**。  
一開始的資料是混亂的，這就代表他們的不純度是很高的，假設節點可以將一批不純資料區分成兩批較純的資料，那麼這個節點就是一個好的節點，透過計算分群前和分群後的不純度，決策樹選擇最能降低不純度的節點作為分支。  
  
常見用於代表不純度的方法有兩種，分別為 (1) 熵 (Shannon entropy) 及 (2) 吉尼係數 (Gini index)。  
這兩個參數的共通點都是在最混亂時其值最高，當資料越純時數值越小，計算節點所能降低的不純度，就可以找出最好的分群方法，他們的計算方法如下。  
  
$$\begin{aligned}
Entropy\ &= - \sum_{j=1}^{c} p_j log(p_j)\\
Gini\ impurity\ &= \sum_{j=1}^{c} p_j (1-p_j) = 1-\sum_{j=1}^{c} p_j^2
\end{aligned}$$   
  
在決定好分群方法後，通常會設定一個臨界值，即要分到每一群內的不純度到多小 (因為分到一群只剩一個也沒有意義)，滿足條件之後就會停止，以此建構一棵樹，但臨界值的選擇很困難，因此也可以透過事後的修剪 (pruning) 來改善效果，即將有問題的節點移除，這部分就不深入討論。  
決策樹有其問題，通常是過度適配 (overfitting) 所導致的，如果臨界值設定的不好也不進行修剪的話，就可能因為某些離群值而造成整個模型過度適配，進而導致實際預測的效果不好。  
  
### Cubist


的概念是將資料



