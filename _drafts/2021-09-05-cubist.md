---
layout: default
categories: Statistic
title:  "Cubist and decision tree (Cubist演算法與決策樹)"
---  
## Cubist and decision tree (Cubist演算法與決策樹)   
2021/09/05   
Cubist是一個在土壤科學 (尤其是數位化土壤繪圖) 相當常見的演算法，他的演算方法是tree-based，所以其實是決策樹 (decision tree) 的延伸，這篇文章從決策樹開始講起，最後帶到這個機器學習上常見的Cubist模型。  
決策樹模型是出自Ross Quinlan這位澳洲學者，演算法包括ID3、C4.5、C5.0等都被廣泛使用 (ID3從1993就開始在使用)，他創立的公司<a href="https://www.rulequest.com/index.html" target="_blank">RuleQuest</a>也將這些方法使用在資料處理上。  
   
*Shoot for the moon. Even if you miss, you'll land among the stars.   
&mdash;Les Brown*   
   
### 決策樹 (decision tree)  
決策樹演算法是監督式學習 (supervised learning) 的一種，監督式學習表示事先給定資料的標籤，再由演算法進行學習並嘗試預測，白話一點來說，就是教導電腦什麼是鯊魚，什麼是鯨魚，然後再讓電腦看圖辨別魚類，反過來說非監督式學習 (unsupervised learning) 就是未給定資料標籤，讓電腦自己從不同魚類照片中依照形態、大小、顏色等特徵來區分。機器學習的監督式學習通常會輸入一組含有許多變數 (特徵) 的資料，例如我們假設一組動物資料。  
  
| 動物 | 有翅膀 | 會游泳 | 會飛 |     
| :---: | :---: | :---: | :---: |      
| 駱駝 | F | F | F |       
| 鯊魚 | F | T | F |      
| 企鵝 | T | T | F |      
| 奇異鳥 | T | F | F |       
{: .tablelines}    
   
決策樹的概念相當簡單，以二元區分的方法，一個節點 (node) 代表一項判別，延伸出兩個分支 (符合與否)，分支又再到下一個節點，直到區分出所有的資料 (圖1)。  
(圖1)  
隨之而來的問題是，我們怎麼挑選節點，如圖1中可以看到"會飛"這個項目完全沒有用到，因為四種動物都不會飛，所以不需要用到這個節點，而"有翅膀"和"會游泳"其實都是很能區分動物的節點，而決策樹選擇節點的方法是透過**不純度 (impurity)**。  
一開始的資料是混亂的，這就代表他們的不純度是很高的，假設節點可以將一批不純資料區分成兩批較純的資料，那麼這個節點就是一個好的節點，透過計算分群前和分群後的不純度，決策樹選擇最能降低不純度的節點作為分支。  
  
常見用於代表不純度的方法有兩種，分別為 (1) 熵 (Shannon entropy) 及 (2) 吉尼係數 (Gini index)。  
這兩個參數的共通點都是在最混亂時其值最高，當資料越純時數值越小，計算節點所能降低的不純度，就可以找出最好的分群方法，他們的計算方法如下。  
  
$$\begin{aligned}
Entropy\ &= - \sum_{j=1}^{c} p_j log(p_j)\\
Gini\ impurity\ &= \sum_{j=1}^{c} p_j (1-p_j) = 1-\sum_{j=1}^{c} p_j^2
\end{aligned}$$   
  
在決定好分群方法後，通常會設定一個臨界值，即要分到每一群內的不純度到多小 (因為分到一群只剩一個也沒有意義)，滿足條件之後就會停止，以此建構一棵樹，但臨界值的選擇很困難，因此也可以透過事後的修剪 (pruning) 來改善效果，即將有問題的節點移除，這部分就不深入討論。  
  
常見的決策樹種類有很多，包括ID3、C4.5、C5.0與CART (Classificaiton and Regression Trees)，這些模型在使用上有一些小差別，但大原則都是遵守前面提到的方法，直到做出一棵樹。  
決策樹有其問題，通常是過度適配 (over-fitting) 所導致的，如果臨界值設定的不好也不進行修剪的話，就可能因為某些離群值而造成整個模型過度適配，進而導致實際預測的效果不好，因此決策樹通常不會是機器學習的首選。不過在針對問題進行解決後，許多奠基於決策樹模型的改良演算法，都逐漸成為機器學習的寵兒，例如隨機森林 (random forest)、M5與今天要提的Cubist。  
  
### M5迴歸樹 (M5 regression tree)   
M5迴歸樹嘗試克服C5.0決策樹中，主要用來預測類別資料 (categorical data) 的這項弱點，相反的M5可以對連續資料 (continuous data) 進行預測。   
當我們要以元素資料預測稻米產量時，決策樹只能告訴你他的產量是"低"、"中"或者"高"，但是迴歸樹卻能直接告訴你4.5這樣的數字，當我們回想使用[多元線性迴歸](https://lloydychuang.github.io/statistic/2021/08/08/regression.html)來預測資料的時候，是將資料建立一條迴歸直線，但我們的資料不必然是遵從同一個規則、同一條迴歸直線，例如圖2的情況，可以很明顯看出分成了 (a)、(b)、(c) 三個區間，(c) 看起來是蠻平的，但是 (a) 跟 (b) 就有趨勢存在。  
(圖2)  
這樣的情況下，如果我們直接使用線性迴歸來計算，得出的結果不佳，但也不能憑直覺劃分區間，因此迴歸樹先使用決策樹來將資料分組，接著在各組內建立線性迴歸方程式，就可以**針對性的建立多條迴歸直線** (圖3)。
(圖3)
   
### Cubist迴歸樹 (Cubist regression tree)   
Cubist迴歸樹就是比M5迴歸樹又更加進化的演算法，從M5的以樹基底 (tree-based) 改變成了Cubist的以**規則基底 (rule-based)**，M5的樹基底包含一連串的if-then判斷，先判斷樣品點位於哪個區段，再決定他需要的迴歸方程式，也就是說每個樣品都需要經過一連串流水線式的判斷，最後才得到他的分區。  
而規則基底則是將判斷式壓縮成扁平並立的規則，輸入後每個規則都會得到一種答案，如圖4。  
(圖4)  
同樣的規則，用樹基底的話要跑完所有判斷式才有一個結果，但規則基底則每個判斷式都有一個結果。除了規則基底的改良外，Cubist迴歸樹還使用一個名為**委員會 (committees)** 的參數
  
  
克服了，決策樹容易受到離群值影響而有過度適配的問題，而且最終結果是將資料依據特徵分類，對於連續型資料較棘手。  
  
Cubist又被稱作迴歸樹，因為可以用於處理連續資料的預測，
，此時Cubist的改良就是將決策樹的分支改為**規則 (rule-based)**，先以決策樹的節點建立if-then的規則，將座標空間劃分成不同小區域，接著再進一步於各小區域內適配
  


的概念是將資料



